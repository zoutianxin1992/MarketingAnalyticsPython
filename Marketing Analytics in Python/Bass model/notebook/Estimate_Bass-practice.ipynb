{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d7da7b3-24df-4442-9b55-d7d5a50e8787",
   "metadata": {},
   "source": [
    "# Use historical sales data to predict future adoptions (Practice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcf7a9f-601c-40db-a7e6-ca31a7fcda8e",
   "metadata": {},
   "source": [
    "## Estimate $p,q, M$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc85fb2b-b64c-41dc-b308-fadcfae336ff",
   "metadata": {},
   "source": [
    "Bass model prediction for new adopters\n",
    "$$\\hat{A}(t) = M\\cdot\\frac{1-exp(-(p+q)t)}{1+\\frac{q}{p}exp(-(p+q)t)}$$ <br>\n",
    "$$\\hat{N}(t) = \\hat{A}(t)-\\hat{A}(t-1)$$ <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db94f2e2-d56a-4c8f-a24e-5187bb1e3545",
   "metadata": {},
   "source": [
    "The GitHub link for the dataset: [3-2-2 BassModelEstimatePQM2.csv](https://github.com/zoutianxin1992/MarketingAnalyticsPython/blob/main/Marketing%20Analytics%20in%20Python/Bass%20model/Dataset/3-2-2%20BassModelEstimatePQM2.csv). The dataset has the product's historical sales for 14 periods. Assuming all sales come from new adoptions, predict and plot the new adoption curve until 30th period.\n",
    "\n",
    "**Remember**: when loading data from GitHub, make sure the url links to the **raw dataset**!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d3c79d-8fc6-4ef6-8c44-bca1a980f75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input your code herer\n",
    "# load packages and datasets, rename datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4330802a-a559-4f82-ae4a-94b6a6e605d5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from matplotlib import pyplot as plt\n",
    "# from scipy.optimize import least_squares                 # package to conduct Nonlinear least square \n",
    "\n",
    "# # import historical data \n",
    "# url = \"https://raw.githubusercontent.com/zoutianxin1992/MarketingAnalyticsPython/main/Marketing%20Analytics%20in%20Python/Bass%20model/Dataset/3-2-2%20BassModelEstimatePQM2.csv\"\n",
    "# df = pd.read_csv(url) \n",
    "# # Rename the variables to t and N_t\n",
    "# df.rename(columns = {df.columns[0]:\"t\",df.columns[1]:\"N\"}, inplace = True)  # \"inplace\" apply the name change to df itself \n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091a797b-a2da-4031-80b8-79c584898c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input your code here\n",
    "# define A_hat(t) and N_hat(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e82cf03-c544-4526-b89f-969905e0a9a4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # define A_hat(t) and N_hat(t)\n",
    "\n",
    "# def A_hat(t,p,q,M):  #t: time, params: the 1*3 array for (p,q,M)\n",
    "#     return M * (1 - np.exp(-(p+q)*t))/(1 + q / p* np.exp(-(p+q)*t))\n",
    "\n",
    "# # define N_hat(t) \n",
    "# def N_hat(t,p,q,M):  \n",
    "#     return A_hat(t,p,q,M) - A_hat(t-1,p,q,M)  # We can use the A_hat function instead of manually typing the formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e071f57-f14b-494b-814b-1ab2a9e8f5e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "The formulae for SSE is \n",
    "$$SSE = \\sum_{t=1}^{T}(N(t)-\\hat{N}(t))^2$$\n",
    "\n",
    "Once we know the prediction errors $e(t) = N(t)-\\hat{N}(t)$ , we can calculate $SSE$. Remember the errors are determined by the Bass parameters ($p,q,M$) and the historical sales data($t,N(t)$). We already know the historical sales data, so the moving parts are $p,q,M$. In other words, the prediction errors $(e(t=1),e(t=2),...e(t=T))$ are a **function** of $p,q,M$.<br> \n",
    "\n",
    "Python's NLS algorithm requires us to tell it how to calculate the prediction error for each period. So, we first construct the prediction errors as a function of $p,q,M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72880c3-58ae-470c-b582-a7d50c0b7d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# construct the prediction_error function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f85cdf7-ac88-4d89-9206-09468609335b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # define prediction errors as a fucntion of p,q,M\n",
    "# T = len(df[\"N\"])   # number of periods for historical data\n",
    "\n",
    "\n",
    "# def prediction_error(params):   # Note that we input p,q,M as a 1*3 array \"params.\" This is required by Python's NLS solver we will use. \n",
    "#     p = params[0]\n",
    "#     q = params[1]\n",
    "#     M = params[2]\n",
    "#     Nhat = [N_hat(t,p,q,M) for t in range(1,T+1)]            # Given p,q,M, generate Bass prediction for each period\n",
    "#     return df[\"N\"] - Nhat                                 # Prediction error for each period\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dcdc85-8020-44e4-b84a-c87e94950672",
   "metadata": {},
   "source": [
    "Our next task is to find $p,q,M$ that minimizes $SSE$ using NLS. We will use `scipy.optimize.least_squares` NLS solver. The input for the solver is `prediction_error`.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e229fe9-3ad5-478b-aae1-24b2a99bed4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "# run NLS to estimate p,q,M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42e4035d-781a-4ba8-949a-7d26fff13ad5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # estimate p,q,M using least_squares\n",
    "# # Bass model requires 0<p, 0<q, M>0, so we need to add the constraints\n",
    "# A_t = sum(df['N'])           # calculate already adopters until period t\n",
    "# params0 = [0.01,0.16,3*A_t]  # initial guess for p,q,M. Required by least_squares\n",
    "# estim_results= least_squares(prediction_error, params0, bounds = (0,np.Inf) )\n",
    "\n",
    "# #########################\n",
    "# # prediction_error: an array of prediction errors for each period\n",
    "# # param0: initial guesses\n",
    "# # bounds: The bounds for p,q,M. In our case p,q,M>0\n",
    "# #########################\n",
    "# estim_results\n",
    "# # Success should be True\n",
    "# # \"x\" is the estimated parameters (what we want).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ff47a0-23c6-4fde-8b7f-b00df81131e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "# store estimation results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c179b7ea-d45f-4c5b-a0a6-bc8bccfeacd4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# p_estim = estim_results.x[0]\n",
    "# q_estim = estim_results.x[1]\n",
    "# M_estim = estim_results.x[2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb94d1f-ba6f-499d-a788-39b6b8125413",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Predict future sales for 30 periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34626bb5-e6ba-4d56-9f1b-1d8f87243db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# predict future sales for 30 periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0d3fa68-f2fc-47a0-9104-c2a968a56e71",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# T_pred = 30  # number of periods for prediction\n",
    "# predictA = [A_hat(t,p_estim,q_estim,M_estim) for t in range(1,T_pred+1)]  # predict already adopters for T periods\n",
    "# predictN = [N_hat(t,p_estim,q_estim,M_estim) for t in range(1,T_pred+1)]  # predict new adopters for T periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50f37d74-96e0-4b46-9682-0f6dd23f8716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Plot predicted new adoptions in 30 periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db3a0370-e903-4710-9ff0-50ec383845ed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Plot the trajectory of new adopters\n",
    "\n",
    "# plt.rcParams['figure.figsize'] = [12,8]  # set figure size to be 12*8 inch\n",
    "# plt.plot(range(1,T_pred+1),predictN)\n",
    "# plt.xticks(range(1,T_pred+1,2), fontsize = 18)\n",
    "# plt.yticks(fontsize = 18)\n",
    "# plt.ylabel(\"New adopters\",fontsize = 18)\n",
    "# plt.xlabel(\"time\", fontsize = 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ec5185-4608-4c65-a5d3-597e51f93c51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
